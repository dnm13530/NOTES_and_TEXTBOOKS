{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/StopWords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     11\u001b[0m stopwords_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/StopWords\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopwords_directory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(stopwords_directory, filename), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/StopWords'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "positive_words = set(line.strip() for line in open('/content/drive/MyDrive/MasterDictionary/MasterDictionary/positive-words.txt', encoding='ISO-8859-1'))\n",
    "negative_words = set(line.strip() for line in open('/content/drive/MyDrive/MasterDictionary/MasterDictionary/negative-words.txt', encoding='ISO-8859-1'))\n",
    "\n",
    "def extract_text_and_headings(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    for tag in soup(['header', 'footer', 'img', 'iframe', 'media']):\n",
    "        tag.extract()\n",
    "\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "    return text, headings\n",
    "\n",
    "def scrape_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        text, headings = extract_text_and_headings(response.content)\n",
    "        return text, headings\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    cleaned_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    pos_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "    neg_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "\n",
    "    polarity = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "    subjectivity = (pos_score + neg_score) / (len(cleaned_words) + 0.000001)\n",
    "\n",
    "    return pos_score, neg_score, polarity, subjectivity\n",
    "\n",
    "def calculate_readability(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    avg_sentence_length = round(len(words) / len(sentences))\n",
    "    complex_word_count = sum(1 for word in words if len(word) > 2 and word.isalnum() and word not in stop_words)\n",
    "    percentage_complex_words = complex_word_count / len(words)\n",
    "\n",
    "    fog_index = round(0.4 * (avg_sentence_length + percentage_complex_words), 4)\n",
    "    avg_words_per_sentence = round(len(words) / len(sentences))\n",
    "\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count\n",
    "\n",
    "def calculate_syllable_per_word(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_syllables = 0\n",
    "\n",
    "    for word in words:\n",
    "        word = re.sub(r'[.,!?]', '', word)\n",
    "        if len(word) > 2 and word.isalnum() and word not in stop_words:\n",
    "            syllables = 0\n",
    "            vowels = 'aeiouAEIOU'\n",
    "            prev_char = None\n",
    "            for char in word:\n",
    "                if char in vowels and (prev_char is None or prev_char not in vowels):\n",
    "                    syllables += 1\n",
    "                prev_char = char\n",
    "            if word.endswith('e'):\n",
    "                syllables -= 1\n",
    "            if syllables == 0:\n",
    "                syllables = 1\n",
    "            total_syllables += syllables\n",
    "\n",
    "    avg_syllables_per_word = total_syllables / len(words)\n",
    "    return avg_syllables_per_word\n",
    "\n",
    "def calculate_personal_pronouns(text):\n",
    "    personal_pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text)\n",
    "    return len(personal_pronouns)\n",
    "\n",
    "def calculate_avg_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    avg_word_length = total_characters / len(words)\n",
    "    return avg_word_length\n",
    "\n",
    "def main():\n",
    "    excel_file = '/content/Input.xlsx'\n",
    "    df = pd.read_excel(excel_file)\n",
    "    text_files_directory = 'extracted_text'\n",
    "    os.makedirs(text_files_directory, exist_ok=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "\n",
    "        text, headings = scrape_url(url.strip())\n",
    "\n",
    "        if text:\n",
    "            text_filename = os.path.join(text_files_directory, f'text_{url_id}.txt')\n",
    "            with open(text_filename, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(text)\n",
    "\n",
    "            pos_score, neg_score, polarity, subjectivity = calculate_sentiment(text)\n",
    "            avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count = calculate_readability(text)\n",
    "            word_count = len(word_tokenize(text))\n",
    "            syllable_per_word = calculate_syllable_per_word(text)\n",
    "            personal_pronouns = calculate_personal_pronouns(text)\n",
    "            avg_word_length = calculate_avg_word_length(text)\n",
    "\n",
    "            df.at[index, 'POSITIVE SCORE'] = pos_score\n",
    "            df.at[index, 'NEGATIVE SCORE'] = neg_score\n",
    "            df.at[index, 'POLARITY SCORE'] = polarity\n",
    "            df.at[index, 'SUBJECTIVITY SCORE'] = subjectivity\n",
    "            df.at[index, 'AVG SENTENCE LENGTH'] = avg_sentence_length\n",
    "            df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
    "            df.at[index, 'FOG INDEX'] = fog_index\n",
    "            df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence\n",
    "            df.at[index, 'COMPLEX WORD COUNT'] = complex_word_count\n",
    "            df.at[index, 'WORD COUNT'] = word_count\n",
    "            df.at[index, 'SYLLABLE PER WORD'] = syllable_per_word\n",
    "            df.at[index, 'PERSONAL PRONOUNS'] = personal_pronouns\n",
    "            df.at[index, 'AVG WORD LENGTH'] = round(avg_word_length, 4)\n",
    "\n",
    "            print(f\"Scores calculated and updated for URL ID {url_id}.\")\n",
    "            print(f\"Extracted text saved to {text_filename}\")\n",
    "\n",
    "    output_file = 'output.xlsx'\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(\"Results saved to\", output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
