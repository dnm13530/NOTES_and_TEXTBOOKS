{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url):\n",
    "    \"\"\"\n",
    "    Scrape all textual data from the given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all text content from the page\n",
    "        page_text = soup.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "        return {\"content\": page_text}  # Return the data as a dictionary\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(tokens):\n",
    "    positive_words = [\"good\", \"great\", \"excellent\", \"positive\", \"happy\", \"awesome\", \"fantastic\"]\n",
    "    negative_words = [\"bad\", \"poor\", \"terrible\", \"negative\", \"sad\", \"awful\", \"horrible\"]\n",
    "    positive_score = sum(word in positive_words for word in tokens)\n",
    "    negative_score = sum(word in negative_words for word in tokens)\n",
    "    return positive_score, negative_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationships(text):\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"nsubj\", \"dobj\"):\n",
    "            relationships.append((token.text, token.head.text, token.dep_))\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping data, please wait...\n",
      "\n",
      "Scraped Product Data:\n",
      "An error occurred: 'name'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to scrape data from a URL and display the results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prompt user for URL input\n",
    "        url = input(\"Enter the product page URL: \").strip()\n",
    "\n",
    "        print(\"\\nScraping data, please wait...\")\n",
    "        product_data = scrape_website(url)\n",
    "\n",
    "        # Check if data was successfully scraped\n",
    "        if not product_data:\n",
    "            print(\"No product data found. Please check the URL or ensure the website structure matches the code.\")\n",
    "        else:\n",
    "            print(\"\\nScraped Product Data:\")\n",
    "            print(f\"Name: {product_data['name']}\")\n",
    "            print(f\"Description: {product_data['description']}\")\n",
    "            print(\"Reviews:\")\n",
    "            for idx, review in enumerate(product_data['reviews'], start=1):\n",
    "                print(f\"  {idx}. {review}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
